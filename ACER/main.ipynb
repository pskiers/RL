{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(nn.Module):\n",
    "    def __init__(self, observation_size, hidden_size, out_size) -> None:\n",
    "        super().__init__()\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.lin1 = nn.Linear(observation_size, hidden_size)\n",
    "        self.mean = nn.Linear(hidden_size, out_size)\n",
    "        self.std = nn.Linear(hidden_size, out_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        out = self.lin1(state)\n",
    "        out = self.activation(out)\n",
    "        mean = self.mean(out)\n",
    "        std = self.std(out)\n",
    "        return mean, torch.abs(std) + 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, gamma, lr, device, observation_size, hidden_size, out_size, b) -> None:\n",
    "        self.policy = ActorModel(observation_size, hidden_size, out_size)\n",
    "        self.policy = self.policy.to(device)\n",
    "        self.gamma = gamma\n",
    "        self.b = b\n",
    "        self.optim = torch.optim.Adam(self.policy.parameters(), lr)\n",
    "\n",
    "    def get_policy(self, observation):\n",
    "        means, std = self.policy(observation)\n",
    "        distribution = torch.distributions.Normal(means, std)\n",
    "        return distribution\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        distribution = self.get_policy(observation)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action, log_prob\n",
    "    \n",
    "    def update(self, rewards, next_state_values, state_values, log_probs, terminals, current_pi, past_pi):\n",
    "        self.optim.zero_grad()\n",
    "        loss = -(rewards + self.gamma * next_state_values * (1 - terminals) - state_values) * log_probs * torch.min(current_pi / past_pi, self.b * torch.ones_like(current_pi))\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, observation_size, hidden_size) -> None:\n",
    "        super().__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.lin1 = nn.Linear(observation_size, hidden_size)\n",
    "        self.lin3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        out = self.lin1(state)\n",
    "        out = self.activation(out)\n",
    "        return self.lin3(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, gamma, lr, b, device, observation_size, hidden_size) -> None:\n",
    "        self.value_function = CriticModel(observation_size, hidden_size)\n",
    "        self.value_function = self.value_function.to(device)\n",
    "        self.gamma = gamma\n",
    "        self.b = b\n",
    "        self.optim = torch.optim.Adam(self.value_function.parameters(), lr)\n",
    "    \n",
    "    def get_state_value(self, state):\n",
    "        return self.value_function(state)\n",
    "    \n",
    "    def update(self, rewards, state_values, next_state_values, terminals, current_pi, past_pi):\n",
    "        self.optim.zero_grad()\n",
    "        loss = ((rewards + (self.gamma * next_state_values)*(1 - terminals) - state_values) * torch.min(current_pi / past_pi, self.b * torch.ones_like(current_pi))).pow(2)\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_length, observation_shape, action_shape):\n",
    "        self.max_length = max_length\n",
    "        self.rewards = torch.zeros(max_length)\n",
    "        self.observations = torch.zeros([max_length, observation_shape])\n",
    "        self.actions = torch.zeros([max_length, action_shape])\n",
    "        self.next_observations = torch.zeros([max_length, observation_shape])\n",
    "        self.pi = torch.zeros(max_length)\n",
    "        self.terminals = torch.zeros(max_length)\n",
    "        self.current_idx = 0\n",
    "        self.added = 0\n",
    "    \n",
    "    def add_experience(self, reward, observation, action, next_observation, pi, terminal):\n",
    "        self.rewards[self.current_idx] = reward\n",
    "        self.observations[self.current_idx] = observation\n",
    "        self.actions[self.current_idx] = action\n",
    "        self.next_observations[self.current_idx] = next_observation\n",
    "        self.pi[self.current_idx] = pi\n",
    "        self.terminals[self.current_idx] = terminal\n",
    "\n",
    "        self.added += 1\n",
    "        self.current_idx = (self.current_idx + 1) % self.max_length\n",
    "    \n",
    "    def get_batch(self, size):\n",
    "        if size > self.max_length:\n",
    "            size = self.max_length\n",
    "        if size > self.added:\n",
    "            size = self.added\n",
    "        indices = torch.randint(min(self.added, self.max_length), (size,))\n",
    "        rewards = self.rewards[indices]\n",
    "        observations = self.observations[indices]\n",
    "        actions = self.actions[indices]\n",
    "        next_obs = self.next_observations[indices]\n",
    "        pi = self.pi[indices]\n",
    "        terminals = self.terminals[indices]\n",
    "\n",
    "        return rewards, observations, actions, next_obs, pi, terminals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env_name, gamma, actor_lr, critic_lr, device, hidden_size_actor, hidden_size_critic, batch_size, buffer_length, b) -> None:\n",
    "        self.device = device\n",
    "        env = gym.make(env_name, max_episode_steps=500, continuous=True)\n",
    "        self.test_env = gym.make(env_name, render_mode=\"human\", max_episode_steps=500, continuous=True)\n",
    "        self.env_wrapper = gym.wrappers.AutoResetWrapper(env)\n",
    "        self.actor = Actor(\n",
    "            gamma=gamma, \n",
    "            lr=actor_lr, \n",
    "            device=device, \n",
    "            observation_size=env.observation_space.shape[0], \n",
    "            hidden_size=hidden_size_actor, \n",
    "            out_size=env.action_space.shape[0], \n",
    "            b=b\n",
    "        )\n",
    "        self.critic = Critic(\n",
    "            gamma=gamma, \n",
    "            lr=critic_lr, \n",
    "            device=device, \n",
    "            observation_size=env.observation_space.shape[0], \n",
    "            hidden_size= hidden_size_critic, \n",
    "            b=b\n",
    "        )\n",
    "        self.replay_buffer = ReplayBuffer(buffer_length, env.observation_space.shape[0], env.action_space.shape[0])\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def learn(self, epochs):\n",
    "        observation, info = self.env_wrapper.reset()\n",
    "        observation = torch.from_numpy(observation).unsqueeze(0).to(self.device)\n",
    "\n",
    "        rewards = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            ep_rewards = []\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, log_prob = self.actor.get_action(observation)\n",
    "                action, log_prob = action.squeeze(), log_prob.squeeze()\n",
    "\n",
    "                new_observation, reward, terminated, truncated, info = self.env_wrapper.step(action.cpu().numpy())\n",
    "                new_observation = torch.from_numpy(new_observation).unsqueeze(0).to(self.device)\n",
    "\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                self.replay_buffer.add_experience(\n",
    "                    reward=reward, \n",
    "                    observation=observation, \n",
    "                    action=action,\n",
    "                    next_observation=new_observation, \n",
    "                    pi=torch.exp(log_prob), \n",
    "                    terminal=done\n",
    "                )\n",
    "\n",
    "                batch_rewards, batch_observations, batch_actions, batch_next_obs, batch_pi, terminals = self.replay_buffer.get_batch(self.batch_size)\n",
    "                batch_rewards = batch_rewards.to(self.device)\n",
    "                batch_observations = batch_observations.to(self.device)\n",
    "                batch_actions = batch_actions.to(self.device)\n",
    "                batch_next_obs = batch_next_obs.to(self.device)\n",
    "                batch_pi = batch_pi.to(self.device)\n",
    "                terminals = terminals.to(self.device)\n",
    "\n",
    "                batch_obs_value = self.critic.get_state_value(batch_observations)\n",
    "                batch_next_obs_value = self.critic.get_state_value(batch_next_obs)\n",
    "                policy = self.actor.get_policy(batch_observations)\n",
    "                batch_log_probs = policy.log_prob(batch_actions).sum(dim=-1)\n",
    "                current_pi = torch.exp(batch_log_probs)\n",
    "\n",
    "                self.critic.update(\n",
    "                    rewards=batch_rewards, \n",
    "                    state_values=batch_obs_value, \n",
    "                    next_state_values=batch_next_obs_value, \n",
    "                    terminals=terminals, \n",
    "                    current_pi=current_pi.detach(),\n",
    "                    past_pi=batch_pi.detach()\n",
    "                )\n",
    "                self.actor.update(\n",
    "                    rewards=batch_rewards.detach(), \n",
    "                    state_values=batch_obs_value.detach(), \n",
    "                    next_state_values=batch_next_obs_value.detach(), \n",
    "                    log_probs=batch_log_probs,\n",
    "                    terminals=terminals.detach(),\n",
    "                    current_pi=current_pi.detach(),\n",
    "                    past_pi=batch_pi.detach()\n",
    "                )\n",
    "                ep_rewards.append(reward)\n",
    "                observation = new_observation\n",
    "                        \n",
    "            rewards.append(sum(ep_rewards))\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                for _ in range(5):\n",
    "                    done = False\n",
    "                    test_observation, info = self.test_env.reset()\n",
    "                    test_observation = torch.from_numpy(test_observation).unsqueeze(0).to(self.device)\n",
    "                    while not done:\n",
    "                        with torch.no_grad():\n",
    "                            action, _ = self.actor.get_action(test_observation)\n",
    "                            action = action.squeeze()\n",
    "                        test_observation, _, terminated, truncated, _ = self.test_env.step(action.cpu().numpy())\n",
    "                        test_observation = torch.from_numpy(test_observation).unsqueeze(0).to(self.device)\n",
    "                        done = terminated or truncated\n",
    "                print(f\"Episode {epoch}\\t Mean reward = {sum(rewards)/len(rewards)}\")\n",
    "                rewards = []\n",
    "        self.test_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    env_name=\"LunarLander-v2\",\n",
    "    gamma=0.99,\n",
    "    actor_lr=0.003,\n",
    "    critic_lr=0.003,\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "    hidden_size_actor=128,\n",
    "    hidden_size_critic=128,\n",
    "    batch_size=256,\n",
    "    buffer_length=100000,\n",
    "    b=10000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5000 [00:15<21:45:07, 15.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\t Mean reward = -398.9090468282223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/5000 [00:26<1:14:50,  1.11it/s]"
     ]
    }
   ],
   "source": [
    "agent.learn(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a38050f8ca24262ead381cbc68a5f26ee5ce1e0f01d0472f124efe95ebd2613"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
