{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(nn.Module):\n",
    "    def __init__(self, observation_size, hidden_size, out_size) -> None:\n",
    "        super().__init__()\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.lin1 = nn.Linear(observation_size, hidden_size)\n",
    "        self.mean = nn.Linear(hidden_size, out_size)\n",
    "        self.std = nn.Linear(hidden_size, out_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        out = self.lin1(state)\n",
    "        out = self.activation(out)\n",
    "        mean = self.mean(out)\n",
    "        std = self.std(out)\n",
    "        return mean, torch.abs(std) + 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, gamma, lr, device, observation_size, hidden_size, out_size, b) -> None:\n",
    "        self.policy = ActorModel(observation_size, hidden_size, out_size)\n",
    "        self.policy = self.policy.to(device)\n",
    "        self.gamma = gamma\n",
    "        self.b = b\n",
    "        self.optim = torch.optim.Adam(self.policy.parameters(), lr)\n",
    "\n",
    "    def get_policy(self, observation):\n",
    "        means, std = self.policy(observation)\n",
    "        distribution = torch.distributions.Normal(means, std)\n",
    "        return distribution\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        distribution = self.get_policy(observation)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action, log_prob\n",
    "    \n",
    "    def update(self, rewards, next_state_values, state_values, log_probs, terminals, current_pi, past_pi):\n",
    "        self.optim.zero_grad()\n",
    "        loss = -(rewards + self.gamma * next_state_values * (1 - terminals) - state_values) * log_probs * torch.min(current_pi / past_pi, self.b * torch.ones_like(current_pi))\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, observation_size, hidden_size) -> None:\n",
    "        super().__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.lin1 = nn.Linear(observation_size, hidden_size)\n",
    "        self.lin3 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        out = self.lin1(state)\n",
    "        out = self.activation(out)\n",
    "        return self.lin3(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, gamma, lr, b, device, observation_size, hidden_size) -> None:\n",
    "        self.value_function = CriticModel(observation_size, hidden_size)\n",
    "        self.value_function = self.value_function.to(device)\n",
    "        self.gamma = gamma\n",
    "        self.b = b\n",
    "        self.optim = torch.optim.Adam(self.value_function.parameters(), lr)\n",
    "    \n",
    "    def get_state_value(self, state):\n",
    "        return self.value_function(state)\n",
    "    \n",
    "    def update(self, rewards, state_values, next_state_values, terminals, current_pi, past_pi):\n",
    "        self.optim.zero_grad()\n",
    "        loss = ((rewards + (self.gamma * next_state_values)*(1 - terminals) - state_values) * torch.min(current_pi / past_pi, self.b * torch.ones_like(current_pi))).pow(2)\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        self.optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_length, observation_shape, action_shape):\n",
    "        self.max_length = max_length\n",
    "        self.rewards = torch.zeros(max_length)\n",
    "        self.observations = torch.zeros([max_length, observation_shape])\n",
    "        self.actions = torch.zeros([max_length, action_shape])\n",
    "        self.next_observations = torch.zeros([max_length, observation_shape])\n",
    "        self.pi = torch.zeros(max_length)\n",
    "        self.log_probs = torch.zeros(max_length)\n",
    "        self.terminals = torch.zeros(max_length)\n",
    "        self.current_idx = 0\n",
    "        self.added = 0\n",
    "    \n",
    "    def add_experience(self, reward, observation, action, next_observation, pi, log_prob, terminal):\n",
    "        self.rewards[self.current_idx] = reward\n",
    "        self.observations[self.current_idx] = observation\n",
    "        self.actions[self.current_idx] = action\n",
    "        self.next_observations[self.current_idx] = next_observation\n",
    "        self.pi[self.current_idx] = pi\n",
    "        self.log_probs[self.current_idx] = log_prob\n",
    "        self.terminals[self.current_idx] = terminal\n",
    "\n",
    "        self.added += 1\n",
    "        self.current_idx = (self.current_idx + 1) % self.max_length\n",
    "    \n",
    "    def get_batch(self, size):\n",
    "        if size > self.max_length:\n",
    "            size = self.max_length\n",
    "        if size > self.added:\n",
    "            size = self.added\n",
    "        indices = torch.randint(min(self.added, self.max_length), (size,))\n",
    "        rewards = self.rewards[indices]\n",
    "        observations = self.observations[indices]\n",
    "        actions = self.actions[indices]\n",
    "        next_obs = self.next_observations[indices]\n",
    "        pi = self.pi[indices]\n",
    "        log_probs = self.log_probs[indices]\n",
    "        terminals = self.terminals[indices]\n",
    "\n",
    "        return rewards, observations, actions, next_obs, pi, log_probs, terminals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env_name, gamma, actor_lr, critic_lr, device, hidden_size_actor, hidden_size_critic, batch_size, buffer_length, b) -> None:\n",
    "        self.device = device\n",
    "        env = gym.make(env_name, max_episode_steps=500, continuous=True)\n",
    "        self.test_env = gym.make(env_name, render_mode=\"human\", max_episode_steps=500)\n",
    "        self.env_wrapper = gym.wrappers.AutoResetWrapper(env)\n",
    "        self.actor = Actor(\n",
    "            gamma=gamma, \n",
    "            lr=actor_lr, \n",
    "            device=device, \n",
    "            observation_size=env.observation_space.shape[0], \n",
    "            hidden_size=hidden_size_actor, \n",
    "            out_size=env.action_space.shape[0], \n",
    "            b=b\n",
    "        )\n",
    "        self.critic = Critic(\n",
    "            gamma=gamma, \n",
    "            lr=critic_lr, \n",
    "            device=device, \n",
    "            observation_size=env.observation_space.shape[0], \n",
    "            hidden_size= hidden_size_critic, \n",
    "            b=b\n",
    "        )\n",
    "        self.replay_buffer = ReplayBuffer(buffer_length, env.observation_space.shape[0], env.action_space.shape[0])\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def learn(self, epochs):\n",
    "        observation, info = self.env_wrapper.reset()\n",
    "        observation = torch.from_numpy(observation).unsqueeze(0).to(self.device)\n",
    "\n",
    "        rewards = []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            ep_rewards = []\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, log_prob = self.actor.get_action(observation)\n",
    "                action, log_prob = action.squeeze(), log_prob.squeeze()\n",
    "\n",
    "                new_observation, reward, terminated, truncated, info = self.env_wrapper.step(action.cpu().numpy())\n",
    "                new_observation = torch.from_numpy(new_observation).unsqueeze(0).to(self.device)\n",
    "\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                self.replay_buffer.add_experience(\n",
    "                    reward=reward, \n",
    "                    observation=observation, \n",
    "                    action=action,\n",
    "                    next_observation=new_observation, \n",
    "                    pi=torch.exp(log_prob), \n",
    "                    log_prob=log_prob, \n",
    "                    terminal=done\n",
    "                )\n",
    "\n",
    "                batch_rewards, batch_observations, batch_actions, batch_next_obs, batch_pi, batch_log_probs, terminals = self.replay_buffer.get_batch(self.batch_size)\n",
    "                batch_rewards = batch_rewards.to(self.device)\n",
    "                batch_observations = batch_observations.to(self.device)\n",
    "                batch_actions = batch_actions.to(self.device)\n",
    "                batch_next_obs = batch_next_obs.to(self.device)\n",
    "                batch_pi = batch_pi.to(self.device)\n",
    "                batch_log_probs = batch_log_probs.to(self.device)\n",
    "                terminals = terminals.to(self.device)\n",
    "\n",
    "                batch_obs_value = self.critic.get_state_value(batch_observations)\n",
    "                batch_next_obs_value = self.critic.get_state_value(batch_next_obs)\n",
    "                policy = self.actor.get_policy(batch_observations)\n",
    "                batch_log_probs = policy.log_prob(batch_actions)\n",
    "                current_pi = torch.exp(batch_log_probs)\n",
    "\n",
    "                self.critic.update(\n",
    "                    rewards=batch_rewards, \n",
    "                    state_values=batch_obs_value, \n",
    "                    next_state_values=batch_next_obs_value, \n",
    "                    terminals=terminals, \n",
    "                    current_pi=current_pi.clone(),\n",
    "                    past_pi=batch_pi\n",
    "                )\n",
    "                self.actor.update(\n",
    "                    rewards=batch_rewards.detach(), \n",
    "                    state_values=batch_obs_value.detach(), \n",
    "                    next_state_values=batch_next_obs_value.detach(), \n",
    "                    log_probs=batch_log_probs,\n",
    "                    terminals=terminals.detach(),\n",
    "                    current_pi=current_pi,\n",
    "                    past_pi=batch_pi.detach()\n",
    "                )\n",
    "                ep_rewards.append(reward)\n",
    "                observation = new_observation\n",
    "                        \n",
    "            rewards.append(sum(ep_rewards))\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                for _ in range(10):\n",
    "                    done = False\n",
    "                    test_observation, info = self.test_env.reset()\n",
    "                    test_observation = torch.from_numpy(test_observation).unsqueeze(0).to(self.device)\n",
    "                    while not done:\n",
    "                        with torch.no_grad():\n",
    "                            action, _ = self.actor.get_action(test_observation)\n",
    "                            action = action.squeeze()\n",
    "                        test_observation, _, terminated, truncated, _ = self.test_env.step(action.cpu().numpy())\n",
    "                        test_observation = torch.from_numpy(test_observation).unsqueeze(0).to(self.device)\n",
    "                        done = terminated or truncated\n",
    "                print(f\"Episode {epoch}\\t Mean reward = {sum(rewards)/len(rewards)}\")\n",
    "                rewards = []\n",
    "        self.test_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    env_name=\"LunarLander-v2\",\n",
    "    gamma=0.99,\n",
    "    actor_lr=0.0003,\n",
    "    critic_lr=0.0003,\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"),\n",
    "    hidden_size_actor=128,\n",
    "    hidden_size_critic=128,\n",
    "    batch_size=64,\n",
    "    buffer_length=1024,\n",
    "    b=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent\u001b[39m.\u001b[39;49mlearn(\u001b[39m5000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [78], line 78\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m current_pi \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(batch_log_probs)\n\u001b[1;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m     71\u001b[0m     rewards\u001b[39m=\u001b[39mbatch_rewards, \n\u001b[1;32m     72\u001b[0m     state_values\u001b[39m=\u001b[39mbatch_obs_value, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     past_pi\u001b[39m=\u001b[39mbatch_pi\n\u001b[1;32m     77\u001b[0m )\n\u001b[0;32m---> 78\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mupdate(\n\u001b[1;32m     79\u001b[0m     rewards\u001b[39m=\u001b[39;49mbatch_rewards\u001b[39m.\u001b[39;49mdetach(), \n\u001b[1;32m     80\u001b[0m     state_values\u001b[39m=\u001b[39;49mbatch_obs_value\u001b[39m.\u001b[39;49mdetach(), \n\u001b[1;32m     81\u001b[0m     next_state_values\u001b[39m=\u001b[39;49mbatch_next_obs_value\u001b[39m.\u001b[39;49mdetach(), \n\u001b[1;32m     82\u001b[0m     log_probs\u001b[39m=\u001b[39;49mbatch_log_probs,\n\u001b[1;32m     83\u001b[0m     terminals\u001b[39m=\u001b[39;49mterminals\u001b[39m.\u001b[39;49mdetach(),\n\u001b[1;32m     84\u001b[0m     current_pi\u001b[39m=\u001b[39;49mcurrent_pi,\n\u001b[1;32m     85\u001b[0m     past_pi\u001b[39m=\u001b[39;49mbatch_pi\u001b[39m.\u001b[39;49mdetach()\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m ep_rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     88\u001b[0m observation \u001b[39m=\u001b[39m new_observation\n",
      "Cell \u001b[0;32mIn [56], line 24\u001b[0m, in \u001b[0;36mActor.update\u001b[0;34m(self, rewards, next_state_values, state_values, log_probs, terminals, current_pi, past_pi)\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(rewards \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_state_values \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m terminals) \u001b[39m-\u001b[39m state_values) \u001b[39m*\u001b[39m log_probs \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mmin(current_pi \u001b[39m/\u001b[39m past_pi, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mones_like(current_pi))\n\u001b[1;32m     23\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(loss)\n\u001b[0;32m---> 24\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/ssne/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ssne/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "agent.learn(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a38050f8ca24262ead381cbc68a5f26ee5ce1e0f01d0472f124efe95ebd2613"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
